#!/usr/bin/env python

"""
Redrock for DESI - MPI entry point
"""
import os
import sys
import argparse
import socket
import numpy as np

from redrock.utils import nersc_login_node, getGPUCountMPI
from redrock.external import desi

# MPI environment availability
have_mpi = None
if nersc_login_node():
    have_mpi = False
else:
    have_mpi = True
    try:
        import mpi4py.MPI as MPI
    except ImportError:
        have_mpi = False
        print ("MPI not available")
        sys.exit(0)

parser = argparse.ArgumentParser(allow_abbrev=False)
parser.add_argument("-i", "--input", type=str, default=None,
        required=True, help="input ASCII file list")
parser.add_argument("--input-dir", type=str, default=None,
        required=False, help="input directory")
parser.add_argument("-o", "--output", type=str, default=None,
        required=True, help="output directory")
parser.add_argument("--gpu", action="store_true",
        required=False, help="use GPUs")
parser.add_argument("--overwrite", action="store_true",
        required=False, help="Overwrite existing output files")
parser.add_argument("--cpu-per-task", type=int, default=32,
        required=False, help="Maximum number of CPUs to use on each input file")
parser.add_argument("--gpuonly", action="store_true",
        required=False, help="Use ONLY GPUs")
#Gather args and any unrecognized args are to be passed to redrock
args, args_to_pass = parser.parse_known_args()
inputdir = None
outdir = args.output
if args.input_dir is not None:
    inputdir = args.input_dir
cpu_per_task = args.cpu_per_task
overwrite = args.overwrite

##Find args to pass to redrock
#args_to_pass = sys.argv[1:]
#args_with_val = ['-i', '-o', '--input-dir', '--input', '--output', '--cpu-per-task']
#args_without_val = ['--gpu', '--overwrite', '--gpuonly']
#for a in args_with_val:
#    if args_to_pass.count(a) > 0:
#        x = args_to_pass.index(a)
#        args_to_pass.pop(x)
#        if x < len(a):
#            args_to_pass.pop(x)
#for a in args_without_val:
#    if args_to_pass.count(a) > 0:
#        args_to_pass.remove(a)

#- global communicator across all nodes
comm = MPI.COMM_WORLD
comm_rank = comm.rank

#Get number of nodes
nhosts = os.getenv('SLURM_NNODES')
if nhosts is None:
    #env var not set, try hostnames
    hostnames = comm.gather(socket.gethostname(), root=0)
    if comm.rank == 0:
        nhosts = len(set(hostnames))
    nhosts = comm.bcast(nhosts, root=0)
else:
    nhosts = int(nhosts)

# GPU configuration
ngpu = 0
gpu_per_node = 0
if args.gpu:
    gpu_per_node = os.getenv('SLURM_GPUS_PER_NODE')
    if gpu_per_node is None:
        #Use utils.getGPUCountMPI which will look at /proc/driver/nvidia/gpus/
        gpu_per_node = getGPUCountMPI()
    else:
        gpu_per_node = int(gpu_per_node)
    ngpu = gpu_per_node*nhosts

#Set GPU nodes
#We want the first gpu_per_node ranks of each host
ranks_per_host = comm.size // nhosts
use_gpu = (comm_rank % ranks_per_host) < gpu_per_node
ncpu_ranks = (comm.size - ngpu -1) // cpu_per_task + 1
if args.gpuonly:
    ncpu_ranks = 0

if comm.rank == 0 and not os.access(outdir, os.F_OK):
    try:
        os.mkdir(outdir)
    except Exception as ex:
        print (ex)
        print ("Error: could not make output directory "+outdir)
        sys.exit(0)

#- read and broadcast input files
inputfiles = None
if comm.rank == 0:
    with open(args.input, 'r') as f:
        inputfiles = f.readlines()
    for i in range(len(inputfiles)):
        inputfiles[i] = inputfiles[i].strip()
        if inputdir is not None:
            inputfiles[i] = inputdir+'/'+inputfiles[i]
inputfiles = comm.bcast(inputfiles, root=0)

#- split subcommunicators
#number of communicators
ncomm = ngpu + ncpu_ranks
if use_gpu:
    myhost = (comm.rank % ranks_per_host) + (comm.rank // ranks_per_host)*gpu_per_node
else:
    myhost = ngpu + (comm.rank - gpu_per_node*(comm.rank // ranks_per_host)) // cpu_per_task
subcomm = comm.Split(myhost)

if comm.rank == 0:
    print("Running "+str(len(inputfiles))+" input files on "+str(ngpu)+" GPUs and "+str(ncomm)+" total procs...")

#- each subcommunicator processes a subset of files
# In --gpuonly mode, CPU procs will not enter this block 
if myhost < ncomm:
    myfiles = np.array_split(inputfiles, ncomm)[myhost]
    for infile in myfiles:
        outfile = os.path.join(outdir, os.path.basename(infile).replace('coadd-', 'redrock-'))
        if (os.access(outfile, os.F_OK)):
            if (overwrite):
                if (subcomm.rank == 0):
                    print ("Warning: overwriting existing file "+str(outfile))
            else:
                if (subcomm.rank == 0):
                    print ("Error: "+str(outfile)+" exists.  Skipping.")
                continue
        opts = ['-i', infile, '-o', outfile]
        if args_to_pass is not None:
            opts.extend(args_to_pass)
        if use_gpu:
            opts.append('--gpu')
        desi.rrdesi(opts, comm=subcomm)
